# Contextual Precision Evaluator
#
# Evaluates whether relevant retrieval nodes are ranked higher than irrelevant ones.
# This metric rewards retrievers that surface relevant content first.
#
# Formula: (1/R) * Î£(Precision@k * r_k) for k=1 to n
# where R = total relevant nodes, r_k = binary relevance at position k
#
# Retrieval context is passed via expected_messages.tool_calls, which
# represents the expected agent behavior (calling a retrieval tool).

execution:
  evaluators:
    - name: contextual_precision
      type: code_judge
      script: [bun, run, ../scripts/contextual-precision.ts]
      # Target access config - allows script to invoke configured targets
      # max_calls should be >= number of retrieval context nodes
      target:
        max_calls: 10

evalcases:
  # Test case 1: Perfect ranking - relevant node first
  # Node 1: Relevant (TypeScript builds on JS)
  # Node 2: Irrelevant (Microsoft, 2012)
  # Node 3: Irrelevant (Python)
  # Score = 1.0 (perfect - only relevant node is ranked first)
  - id: perfect-ranking
    expected_outcome: TypeScript is based on JavaScript
    input_messages:
      - role: user
        content: What programming language is TypeScript based on?
    expected_messages:
      - role: assistant
        tool_calls:
          - tool: vector_search
            input:
              query: TypeScript based on
            output:
              results:
                - "TypeScript is a strongly typed programming language that builds on JavaScript."
                - "TypeScript was developed by Microsoft and first released in 2012."
                - "Python is a high-level programming language known for its readability."
      - role: assistant
        content: TypeScript is a superset of JavaScript.

  # Test case 2: Multiple relevant nodes ranked first
  # Node 1: Relevant (Paris is capital)
  # Node 2: Irrelevant (Eiffel Tower)
  # Node 3: Relevant (City of Light = Paris)
  # Precision@1 = 1/1 = 1.0, Precision@3 = 2/3 = 0.667
  # Score = (1/2) * (1.0 + 0.667) = 0.833
  - id: mixed-ranking
    expected_outcome: The capital of France is Paris.
    input_messages:
      - role: user
        content: What is the capital of France?
    expected_messages:
      - role: assistant
        tool_calls:
          - tool: vector_search
            input:
              query: capital of France
            output:
              results:
                - "Paris is the capital and most populous city of France."
                - "The Eiffel Tower was built in 1887."
                - "Paris is often referred to as the City of Light."
      - role: assistant
        content: Paris is the capital of France.

  # Test case 3: Worst case - only relevant node is last
  # Node 1: Irrelevant (grass)
  # Node 2: Irrelevant (roses)
  # Node 3: Relevant (sky is blue)
  # Precision@3 = 1/3 = 0.333
  # Score = (1/1) * 0.333 = 0.333
  - id: relevant-node-last
    expected_outcome: The sky is blue
    input_messages:
      - role: user
        content: What color is the sky?
    expected_messages:
      - role: assistant
        tool_calls:
          - tool: vector_search
            input:
              query: color of sky
            output:
              results:
                - "Grass is typically green in color."
                - "Roses can be red, pink, or white."
                - "The sky appears blue due to Rayleigh scattering of sunlight."
      - role: assistant
        content: The sky appears blue during the day.
