---
title: Rubrics
description: Structured evaluation criteria with weights
sidebar:
  order: 3
---

Rubrics are defined with `assert` entries using `type: rubrics`. They support binary checklist grading and score-range analytic grading.

## Basic Usage

The simplest form — each string becomes a required criterion:

```yaml
tests:
  - id: quicksort-explain
    criteria: Explain how quicksort works
    input: Explain quicksort algorithm
    assert:
      - type: rubrics
        criteria:
          - Mentions divide-and-conquer approach
          - Explains partition step
          - States time complexity
```

## Checklist Mode

For fine-grained control, use rubric objects with weights and requirements:

```yaml
assert:
  - type: rubrics
    criteria:
      - id: core-concept
        outcome: Explains divide-and-conquer
        weight: 2.0
        required: true
      - id: partition
        outcome: Describes partition step
        weight: 1.5
      - id: complexity
        outcome: States O(n log n) average time
        weight: 1.0
```

### Rubric Object Fields

| Field | Default | Description |
|-------|---------|-------------|
| `id` | Auto-generated | Unique identifier for the criterion |
| `outcome` | — | Description of what to check |
| `weight` | `1.0` | Relative importance for scoring |
| `required` | `false` | If true, failing this criterion fails the entire eval |
| `required_min_score` | — | Minimum score threshold (score-range mode) |
| `score_ranges` | — | Score range definitions (analytic mode) |

## Score-Range Mode (Analytic)

For quality gradients instead of binary pass/fail, use score ranges:

```yaml
assert:
  - type: rubrics
    criteria:
      - id: accuracy
        outcome: Provides correct answer
        weight: 2.0
        score_ranges:
          0: Completely wrong
          3: Partially correct with major errors
          5: Mostly correct with minor issues
          7: Correct with minor omissions
          10: Perfectly accurate and complete
```

Each criterion is scored 0–10 by the LLM judge with granular feedback.

## Scoring

### Checklist Mode

```
score = sum(satisfied_weights) / sum(total_weights)
```

### Score-Range Mode

```
score = sum(criterion_score / 10 * weight) / sum(total_weights)
```

### Verdicts

| Verdict | Score |
|---------|-------|
| `pass` | ≥ 0.8 |
| `borderline` | ≥ 0.6 |
| `fail` | < 0.6 |

## Auto-Generate Rubrics

Generate rubrics from expected outcomes:

```bash
agentv generate rubrics evals/my-eval.yaml
```

This analyzes each test's `criteria` and creates structured rubric criteria.

## Combining with Other Evaluators

Rubrics work alongside code and LLM judges:

```yaml
tests:
  - id: code-quality
    criteria: Generates correct, clean Python code
    input: Write a fibonacci function
    assert:
      - type: rubrics
        criteria:
          - Returns correct values for n=0,1,2,10
          - Uses meaningful variable names
          - Includes docstring
      - name: syntax_check
        type: code_judge
        script: ./validators/check_python.py
```
