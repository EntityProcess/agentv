# ACE Optimization Example - Incident Severity Classification
# This eval file is designed to work with ACE optimization
# Key features for ACE:
# 1. System prompts that elicit step-by-step reasoning (captured as generator_reasoning)
# 2. Clear expected outcomes for the Reflector to analyze
# 3. Diverse examples covering edge cases that benefit from playbook learning

$schema: agentv-eval-v2
description: Incident severity classification eval designed for ACE optimization

target: default

evalcases:
  # ==========================================
  # Basic severity classification with explicit reasoning
  # ==========================================
  - id: severity-global-maintenance
    conversation_id: incident-triage
    
    outcome: |
      Assistant correctly classifies as 'medium' severity despite global scope,
      because it's planned maintenance with acceptable degradation. Reasoning should
      distinguish between unplanned outages and planned maintenance windows.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Global search results delayed during planned reindex
          Impact: Catalog searchable but updates appear 20 mingit utes late
          Scope: global
          Signals: Maintenance ticket CAB-512 approved, no customer complaints
    
    expected_messages:
      - role: assistant
        content: |
          Let me analyze this incident step-by-step:
          
          1. Scope Analysis: Global scope typically indicates high severity
          2. Planned vs Unplanned: This is approved planned maintenance (CAB-512)
          3. Business Impact: Catalog remains searchable, only update latency affected
          4. Customer Impact: No customer complaints reported
          
          Severity: medium
          
          Rationale: While global in scope, this is planned maintenance with acceptable 
          degradation. Core functionality (search) remains operational. The absence of 
          customer complaints and pre-approval reduces severity from what would otherwise 
          be high for a global issue.

  - id: severity-internal-low-priority
    conversation_id: incident-triage
    
    outcome: |
      Assistant correctly classifies as 'low' severity for internal-only impact
      with no customer exposure. Reasoning should explain why internal scope
      reduces urgency even when functionality is completely down.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Internal analytics dashboard shows stale finance data
          Impact: Financial planning team delayed; no external exposure
          Scope: internal
          Signals: ETL job lagging 60 minutes, status yellow
    
    expected_messages:
      - role: assistant
        content: |
          Let me analyze this incident step-by-step:
          
          1. Scope Analysis: Internal-only, no customer-facing impact
          2. Business Impact: Planning team inconvenienced but no revenue/compliance risk
          3. Operational Impact: ETL lag is concerning but not critical
          4. External Exposure: Explicitly stated as none
          
          Severity: low
          
          Rationale: Internal scope with no customer exposure, no revenue impact, and 
          no compliance risk justifies low severity. While the finance team is affected, 
          the 60-minute lag is an inconvenience rather than a blocker. Status yellow 
          (vs red) confirms this is degraded but functional.

  # ==========================================
  # Advanced Cases: Ambiguous Signals & Nuanced Judgment
  # These cases showcase ACE's ability to learn patterns beyond simple rules
  # ==========================================
  
  - id: severity-contextual-timing-black-friday
    conversation_id: incident-triage
    
    outcome: |
      Assistant recognizes that identical technical issue has different severity based on timing.
      3% error rate during Black Friday is 'critical', same rate during off-peak is 'medium'.
      Must reference business context and opportunity cost.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Checkout service returning 503 errors at 3% rate
          Impact: ~300 failed transactions per minute
          Scope: global
          Signals: Black Friday (Nov 29), 10x normal traffic volume, revenue trending 40% below 
          forecast, competitor site performance normal
    
    expected_messages:
      - role: assistant
        content: |
          **Critical revenue impact during peak business period**
          
          A 3% error rate would normally be 'medium' severity, but context elevates this to critical:
          
          - Timing: Black Friday - highest revenue day of the year
          - Opportunity cost: 300 failed txns/min × ~$50 avg = $15K/min revenue loss
          - Volume multiplier: 10x normal traffic means 3% affects 10x more customers
          - Competitive pressure: Competitors functioning normally = customers will switch
          - Forecast gap: 40% below target suggests systemic issue beyond normal variance
          
          **Severity: critical**
          
          Business context transforms a manageable technical issue into an existential threat. 
          The same 3% error rate during off-peak (affecting 30 txns/min, $1.5K/min) would be 
          'medium' severity. Black Friday has ~2-hour window for maximum revenue capture - 
          every minute counts.
          
          All hands needed: This is a "stop the line" moment.

  - id: severity-post-deployment-correlation
    conversation_id: incident-triage
    
    outcome: |
      Assistant elevates severity and confidence based on deployment correlation.
      Recent code push changes this from 'investigate' to 'rollback immediately'.
      Must recognize deployment timing as strong root cause signal.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: API gateway 500 errors increased from 0.1% to 2.5%
          Impact: ~500 requests/min failing across all endpoints
          Scope: global
          Signals: Payment service v2.4.1 deployed 8 minutes ago, error spike began 30 seconds 
          post-deployment, no infrastructure changes, previous version (v2.4.0) stable for 2 weeks
    
    expected_messages:
      - role: assistant
        content: |
          **Deployment-induced regression with clear rollback path**
          
          Strong temporal correlation between deployment and error spike indicates code regression:
          
          - Error baseline: 0.1% (10 errors/min) - consistently stable
          - Post-deployment: 2.5% (500 errors/min) - 25x increase
          - Timing precision: Spike began 30 seconds after deployment completion
          - Stable history: v2.4.0 ran cleanly for 2 weeks
          - Infrastructure: No platform changes - isolates to application code
          
          **Severity: high**
          
          Root cause confidence: 95%+ (deployment correlation)
          Remediation path: Clear and fast (rollback to v2.4.0)
          
          The deployment correlation transforms this from "investigate obscure issue" to 
          "rollback immediately." While we could spend hours debugging v2.4.1, the smart 
          play is instant rollback to restore service, then debug offline.
          
          Recommended action: Initiate rollback to v2.4.0 now, conduct post-mortem on 
          v2.4.1 changes after service restoration.

  - id: severity-conflicting-signals-compliance
    conversation_id: incident-triage
    
    outcome: |
      Assistant navigates conflicting signals (internal scope vs regulatory risk).
      Must recognize that regulatory/compliance issues override normal scope rules.
      Single-user + internal + compliance = high severity.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: CFO unable to access SOX compliance dashboard for quarterly filing
          Impact: Regulatory filing blocked, potential SEC penalties $500K-$5M for missed deadline
          Scope: single-user, internal
          Signals: Filing deadline in 4 hours, no manual workaround available, legal escalation 
          opened, dashboard shows "403 Forbidden" due to expired certificate
    
    expected_messages:
      - role: assistant
        content: |
          **Regulatory compliance trumps scope considerations**
          
          Normal triage rules (single-user + internal = low) break down under regulatory pressure:
          
          Scope analysis:
          - Technical: Single user (CFO only)
          - Operational: Internal system (no customer impact)
          
          Risk analysis:
          - Financial: $500K-$5M in SEC penalties
          - Legal: Regulatory filing deadline in 4 hours
          - Reputational: Public disclosure of compliance failure
          - Organizational: C-suite blocked, legal escalation active
          
          **Severity: high**
          
          Compliance and regulatory issues create an exception to standard severity rubrics. 
          A single internal user blocked from a compliance-critical system carries greater 
          business risk than a regional customer-facing outage with workarounds.
          
          The expired certificate is trivial to fix (minutes), but the compliance deadline 
          is immovable. This is a "drop everything" scenario despite limited scope.

  - id: severity-gradual-degradation-memory-leak
    conversation_id: incident-triage
    
    outcome: |
      Assistant recognizes slow-moving incident requires immediate action despite 
      current functionality appearing normal. Must project future state based on trend.
      Pattern: gradual degradation is harder to detect but equally dangerous.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Order processing service memory usage increasing linearly
          Impact: Currently processing normally, 99.99% success rate maintained
          Scope: global
          Signals: Memory usage 45% → 78% over past 6 hours (linear trend), projected OOM in 
          90 minutes, service restarts clear memory but orders in-flight fail, peak shopping 
          hours starting in 60 minutes
    
    expected_messages:
      - role: assistant
        content: |
          **Time-critical intervention needed despite current stability**
          
          Current state looks healthy (99.99% success), but trajectory analysis shows crisis ahead:
          
          Trend analysis:
          - Memory growth: 45% → 78% in 6 hours = 5.5% per hour
          - Projected OOM: 90 minutes (before peak shopping hours)
          - Peak traffic: Begins in 60 minutes (will accelerate memory growth)
          
          Risk factors:
          - Restart = in-flight order failures (customer impact)
          - Peak timing: Worst possible moment for service disruption
          - Global scope: No failover region available
          
          **Severity: high**
          
          This is a "slow-motion incident" - the frog-boiling problem. Current metrics don't 
          reflect urgency, but we have 60-90 minute window to act before guaranteed outage 
          during peak hours.
          
          Strategy: Controlled intervention NOW (add capacity, investigate leak) beats emergency 
          restart during peak traffic. Similar to flight crews addressing gradual cabin pressure 
          loss before passengers notice symptoms.

  - id: severity-global-maintenance-noisy
    conversation_id: incident-triage
    
    outcome: |
      Assistant classifies as 'medium' despite user reports and noisy alerts.
      Reasoning must filter signal from noise, linking latency to the active maintenance 
      window and confirming that core availability is preserved. Must identify red herrings.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: "Search is broken, spinning forever" - reported by 12 internal users and 3 customers
          Impact: Search latency avg 4500ms (normal 200ms), zero 500 errors
          Scope: global
          Signals: 
            - Active Change Request: CHG-998 (Elasticsearch Re-indexing) - Status: In Progress
            - Alert: "High CPU on ES-Nodes" (Silenced/Muted)
            - Slack: Support team asking "Is search down?"
            - Health Check: /health endpoint returning 200 OK
            - Ancillary Alert: "Marketing-Site-Analytics" reporting data lag (unrelated)
    
    expected_messages:
      - role: assistant
        content: |
          **Noise filtering required - separating expected degradation from real incidents**
          
          Signal analysis:
          - User complaints: 15 reports of "broken" search (alarming language)
          - Latency spike: 4500ms (22x degradation from 200ms baseline)
          - Availability: 100% (zero errors, health check passing)
          
          Causality chain:
          - Active CHG-998 (Elasticsearch Re-indexing) directly explains CPU spike and latency
          - High CPU alert was pre-emptively silenced (change team expected this)
          - Health endpoint green confirms service logic functioning correctly
          
          Red herrings identified:
          - Marketing analytics lag is unrelated (separate data pipeline)
          - Support team panic ("Is search down?") doesn't match technical reality (search is slow, not down)
          
          **Severity: medium**
          
          This is a known, planned degradation within an approved maintenance window. While 
          customer-facing and generating complaints, the service is available and functioning 
          correctly - just slowly. User reports reflect perception ("broken") not technical 
          reality (degraded performance during maintenance).
          
          Action required: Communication to support team about maintenance window, not emergency 
          engineering response. Similar to planned database migration scenarios where performance 
          temporarily degrades but availability is maintained.

  - id: severity-data-integrity-split-brain
    conversation_id: incident-triage
    
    outcome: |
      Assistant identifies critical data integrity risk despite green availability dashboards.
      Must classify as 'critical' due to silent data corruption and financial record 
      discrepancies. Reasoning must prioritize data correctness over availability metrics.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Finance reports discrepancies in daily ledger
          Impact: "Orders Table" row count does not match "Payments Table" count for last 2 hours, Delta: 150 records
          Scope: global (data layer)
          Signals: 
            - API Availability: 100% (All Green)
            - Latency: Normal (120ms)
            - Database Logs: "Constraint Violation" warnings appearing on Write Replica 3
            - Replication Lag: Spiked to 5000ms on Replica 3 only
    
    expected_messages:
      - role: assistant
        content: |
          **Critical data integrity failure hidden behind green dashboards**
          
          Symptom analysis:
          - 150-record discrepancy between Orders and Payments tables
          - Constraint violations on Write Replica 3
          - Isolated replication lag spike (5000ms) on single replica
          
          Hidden failure mode - "Split Brain" scenario:
          - API reports 100% availability because writes to Primary succeed
          - Reads may be hitting lagging/corrupted Replica 3 (depending on load balancer routing)
          - Application sees inconsistent state: orders without payments, payments without orders
          - Financial reconciliation impossible with desynchronized data
          
          Dashboard deception:
          - Availability metrics (100%) measure "can we process requests?" not "is data correct?"
          - Latency normal (120ms) because corrupt replica responds quickly with wrong data
          - Traditional monitoring focused on uptime misses silent corruption
          
          **Severity: critical**
          
          Data integrity violations are the highest severity class - worse than downtime. The 
          system is "lying" (reporting green) while permanently losing/corrupting financial records. 
          150 mismatched transactions represent real money and regulatory reporting obligations.
          
          Immediate actions required:
          1. Stop write traffic OR isolate Replica 3 from read pool
          2. Prevent further corruption before investigating root cause
          3. Begin data reconciliation process for affected 2-hour window
          
          This demonstrates why data correctness must override availability in severity calculations. 
          A honest outage (returning errors) is preferable to silent corruption (returning wrong data).
  - id: severity-regional-api-sunset-migration
    conversation_id: incident-triage
    
    outcome: |
      Assistant classifies as 'medium' despite customer complaints and regional impact.
      Must recognize this is planned API deprecation with advance notice, not an unplanned outage.
      Challenging because customer distress signals conflict with planned migration context.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: EU partners reporting "API authentication failed" errors on legacy v1 endpoints
          Impact: 15 partners affected, migration to v2 endpoints incomplete
          Scope: regional (EU)
          Signals: Sunset notice sent 90 days ago (email confirmations logged), v1 deprecation 
          scheduled for today, v2 endpoints fully operational, migration guide published, 
          partners acknowledged sunset but delayed migration work

    expected_messages:
      - role: assistant
        content: |
          **Planned deprecation with adequate notice - not an emergency incident**
          
          Initial assessment might suggest high severity (15 partners impacted, authentication failing),
          but context reveals this is a managed sunset:
          
          Evidence of planned change:
          - Sunset notice: 90 days advance warning (industry standard: 30-60 days)
          - Partner acknowledgment: Email confirmations on record
          - Alternative available: v2 endpoints fully operational
          - Documentation: Migration guide published
          
          Partner responsibility assessment:
          - Partners acknowledged sunset timeline
          - Partners delayed migration work despite adequate notice
          - This is a procurement/project management failure on partner side, not platform failure
          
          **Severity: medium**
          
          While 15 partners are affected, this is a planned and communicated change. The impact 
          is real but expected and avoidable. Severity reflects the need to support migration 
          (customer success concern) without treating it as an emergency platform incident.
          
          Response strategy: Accelerate partner migration support, not emergency rollback. Similar 
          to library deprecations where breaking changes are announced but adoption lags.

  - id: severity-weekend-batch-job-failure
    conversation_id: incident-triage
    
    outcome: |
      Assistant classifies as 'low' despite complete job failure and data staleness.
      Must recognize that weekend batch jobs for Monday reports have recovery window.
      Challenging because "complete failure" sounds critical, but temporal context matters.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Weekend sales aggregation batch job failed (Saturday 2AM run)
          Impact: Executive dashboard showing Friday's data, Monday reports will be stale
          Scope: internal (analytics/reporting only)
          Signals: It's Saturday 8AM, business reopens Monday 9AM (37 hours available), 
          job typically takes 45 minutes to rerun, no customer-facing impact, 
          failure cause identified (disk space - easily fixable)

    expected_messages:
      - role: assistant
        content: |
          **Temporal context creates recovery window - urgency misaligned with impact**
          
          Failure assessment:
          - Technical severity: Complete batch job failure (sounds critical)
          - Data impact: Monday reports will show stale Friday data (concerning)
          
          Temporal mitigation factors:
          - Current time: Saturday 8AM
          - Business hours resume: Monday 9AM (37 hours away)
          - Job runtime: 45 minutes
          - Recovery window: 37 hours to fix a 45-minute job = 49x safety margin
          - Root cause: Disk space (fixable in minutes, not hours)
          
          **Severity: low**
          
          The complete failure description is misleading without time context. We have ample 
          recovery time before business impact occurs. This is "fix Monday morning" not 
          "page the on-call engineer Saturday." The failure's timing (weekend) transforms 
          what could be a medium-severity weekday issue into low-severity with comfortable 
          recovery buffer.
          
          Appropriate response: Schedule fix for Monday morning standup, not weekend emergency work.
          Similar to backup failures discovered Saturday for weekly retention - addressable 
          within business cycle.

  - id: severity-partial-region-payment-gateway
    conversation_id: incident-triage
    
    outcome: |
      Assistant classifies as 'critical' despite only 8% error rate and partial region impact.
      Must recognize payment failures = revenue loss, and lack of fallback/retry makes this urgent.
      Challenging because percentages seem low, but payment domain context elevates severity.
    
    input_messages:
      - role: system
        content: |
          You are doing first-pass incident triage.
          Think step-by-step and explain your reasoning before providing the severity classification.
          Always justify your severity assessment based on scope and business impact.
      - role: user
        content: |
          Classify the severity of this incident:
          
          Ticket: Payment gateway returning "processor timeout" for some APAC transactions
          Impact: 8% of APAC payment attempts failing, customers abandoning carts
          Scope: partial regional (APAC, intermittent)
          Signals: No automatic retry mechanism, no fallback processor, error rate climbing 
          from 2% → 8% over last 20 minutes, trend continuing, payments are one-shot 
          (failed = lost sale), support tickets spiking, Twitter mentions starting

    expected_messages:
      - role: assistant
        content: |
          **Payment failures with cart abandonment = direct revenue hemorrhage**
          
          Initial numeric assessment appears moderate:
          - 8% error rate (92% success seems acceptable?)
          - Partial regional scope (not global)
          - Intermittent failures (not total outage)
          
          Payment domain context transforms severity:
          - Payment failures are permanent: No retry, no fallback, no second chance
          - Failed payment = abandoned cart = lost revenue (not degraded experience)
          - Error trend: 2% → 8% in 20 minutes, continuing climb
          - Projected trajectory: Could reach 15-20% in next hour
          - Customer behavior: Payment failure triggers immediate competitor research
          
          Architectural vulnerability:
          - Single processor (no redundancy)
          - No automatic retry logic
          - One-shot failure mode
          
          Brand damage signals:
          - Support ticket spike (reactive complaints)
          - Twitter mentions (proactive public complaints - brand risk)
          
          **Severity: critical**
          
          An 8% payment error rate is catastrophic in e-commerce, even if 92% succeed. Unlike 
          content delivery (where retry helps) or search (where degradation is tolerable), 
          payment failures are binary: success = revenue, failure = $0 + angry customer.
          
          The climbing trend (2% → 8%) indicates systemic issue, not random variance. Immediate 
          intervention required before error rate reaches double digits and social media 
          amplifies brand damage. This is "stop the revenue bleeding" severity.