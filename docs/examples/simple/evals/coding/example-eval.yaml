# AgentV Eval Schema Example
# Demonstrates schema features with real file references and minimal redundancy

$schema: agentv-eval-v2
description: Example showing basic features, conversation threading, multiple evaluators

# File-level default target (optional)
# Can be overridden per eval case via execution.target
target: default

evalcases:
  # ==========================================
  # Example 1: Basic V2 features with file references
  # Demonstrates: input_messages, expected_messages, file references, array content format
  # ==========================================
  - id: code-review-javascript
    
    outcome: |
      Assistant provides helpful code analysis and mentions SUPERSECRET_INSTRUCTION_MARKER_JAVASCRIPT
    
    input_messages:
      - role: system
        content: You are an expert software developer who provides clear, concise code reviews.
      - role: user
        content:
          # Content can be a string or an array of content blocks
          # Array format supports mixed text and file references
          - type: text
            value: |-
              Please review this JavaScript function:
              
              ```javascript
              function calculateTotal(items) {
                let total = 0;
                for (let i = 0; i < 0; i++) {
                  total += items[i].price * items[i].quantity;
                }
                return total;
              }
              ```
          # File references are resolved relative to the eval file directory
          # Files ending in .instructions.md are treated as guidelines
          - type: file
            value: ../../prompts/javascript.instructions.md
          - type: file
            value: ../local-cli/attachments/example.txt
    
    expected_messages:
      - role: assistant
        content: |-
          The function has a critical bug in the loop condition. Here's my analysis (SUPERSECRET_INSTRUCTION_MARKER_JAVASCRIPT):
          
          **Critical Issue:**
          - Loop condition `i < 0` means the loop never executes (should be `i < items.length`)
          
          **Suggestions:**
          - Fix the loop: `for (let i = 0; i < items.length; i++)`
          - Consider using `reduce()` for a more functional approach
          - Add input validation for edge cases

  # ==========================================
  # Example 2: Advanced features - conversation_id, multiple evaluators
  # Demonstrates: conversation threading, execution config, target override, evaluators
  # Note: Optimization (ACE, etc.) is configured separately in opts/*.yaml files
  # ==========================================
  - id: code-gen-python-comprehensive
    
    # conversation_id represents the full conversation that may be split into multiple eval cases
    # Most commonly, eval cases test the final response, but could also test intermediate turns
    conversation_id: python-code-generation
    
    outcome: AI generates correct Python function with proper error handling, type hints, and mentions SUPERSECRET_INSTRUCTION_MARKER_PYTHON

    execution:
      # Override file-level target for this specific eval case
      target: azure_base
      
      # Multiple evaluators - supports both code-based and LLM judges
      evaluators:
        - name: keyword_check
          type: code  # Code evaluators handle regex, keywords, linting, etc.
          script: uv run check_python_keywords.py
          cwd: ../../evaluators/scripts  # Working directory for script execution
        - name: code_correctness
          type: llm_judge  # LLM-based evaluation
          prompt: ../../evaluators/prompts/code-correctness-judge.md
    
    input_messages:
      - role: system
        content: You are a code generator that follows specifications exactly.
      - role: user
        content:
          - type: text
            value: |-
              Create a Python function that:
              1. Takes a list of integers
              2. Returns the second largest number
              3. Handles edge cases (empty list, single item, duplicates)
              4. Raises appropriate exceptions for invalid input
          # Reference to real instruction file
          - type: file
            value: ../../prompts/python.instructions.md
    
    expected_messages:
      - role: assistant
        content:
          - type: file
            value: ./snippets/python-second-largest.md

  # ==========================================
  # Example 3: Outcome-only evaluation (no reference answer)
  # Demonstrates: Evaluating based on criteria alone, useful for creative/open-ended tasks
  # Use case: When there's no single "correct" answer, only quality criteria to meet
  # ==========================================
  - id: feature-proposal-brainstorm
    
    # Outcome describes what makes a good response without providing a reference answer
    # The LLM judge evaluates whether the response meets these criteria
    outcome: |-
      Assistant generates 3-5 creative feature ideas for a mobile fitness app. Each idea should:
      1. Address a specific user pain point
      2. Be technically feasible with current mobile technology
      3. Include a brief value proposition (1-2 sentences)
      4. Be distinct from the others (no duplicate concepts)
      Ideas should be innovative but practical, avoiding generic suggestions like "add social sharing."
    
    input_messages:
      - role: system
        content: You are a product strategist specializing in mobile health and fitness applications.
      - role: user
        content: |-
          We're developing a mobile fitness app and need fresh feature ideas that would differentiate us from competitors.
          Our target users are busy professionals aged 25-45 who struggle to maintain consistent workout routines.
          
          Please brainstorm 3-5 innovative features we should consider building.
    
    # Note: No expected_messages - evaluation is purely based on whether outcome criteria are met
    # The LLM judge will assess: creativity, technical feasibility, value propositions, and distinctiveness
